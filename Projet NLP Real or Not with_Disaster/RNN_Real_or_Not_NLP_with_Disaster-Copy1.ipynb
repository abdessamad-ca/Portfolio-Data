{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n",
      "0    57.034021\n",
      "1    42.965979\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"target\"].value_counts())\n",
    "print(dataset[\"target\"].value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f5b2cdeac8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(dataset[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text=dataset.drop(columns=[\"id\",\"keyword\",\"location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1\n",
       "1                Forest fire near La Ronge Sask. Canada       1\n",
       "2     All residents asked to 'shelter in place' are ...       1\n",
       "3     13,000 people receive #wildfires evacuation or...       1\n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1\n",
       "...                                                 ...     ...\n",
       "7608  Two giant cranes holding a bridge collapse int...       1\n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1\n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1\n",
       "7611  Police investigating after an e-bike collided ...       1\n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1\n",
       "\n",
       "[7613 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Remove HTML\n",
    "    -Tokenization + Remove punctuation\n",
    "    -Remove stop words\n",
    "    -Lemmatization or Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punct = string.punctuation\n",
    "# stopwords = nltk.corpus.stopwords.words()\n",
    "# wl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-8219effadba8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-8219effadba8>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    from string import string.punctuation\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# from string import string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Abdessamad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_preprocessing(text):\n",
    "#     text = text.lower()\n",
    "#     punct = \"\".join([word for word in text if word not in string.punctuation)]\n",
    "#     token=RegexpTokenizer(\"r\\w+\")\n",
    "#     token_txt= [token.tokenize(i)  for i in text] \n",
    "#     stopw=[i for i in text if i not in stopwords.words(\"english\")]\n",
    "#     lemmat =WordNetLemmatizer()\n",
    "#     lemmat_txt= [lemmat.lemmatize(i) for i in text]\n",
    "#     stemm = PorterStemmer()\n",
    "#     stemm_txt = \" \".join([stemm.stem(i) for i in text])\n",
    "#     return stemm_txt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "#     token=RegexpTokenizer(r\"\\w+\")\n",
    "#     text= [token.tokenize(i)  for i in text] \n",
    "#     text=[i for i in text if i not in stopwords.words(\"english\")]\n",
    "#     lemmat =WordNetLemmatizer()\n",
    "#     text= [lemmat.lemmatize(i) for i in text]\n",
    "#     stemm = PorterStemmer()\n",
    "#     text = \" \".join([stemm.stem(i) for i in text])\n",
    "\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text[\"text_clean\"]  = dataset_text['text'].apply(lambda x: text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          text_clean  \n",
       "0  our deeds are the reason of this earthquake ma...  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  all residents asked to shelter in place are be...  \n",
       "3  13000 people receive wildfires evacuation orde...  \n",
       "4  just got sent this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=RegexpTokenizer(r\"\\w+\")\n",
    "dataset_text[\"text_clean\"] = dataset_text[\"text_clean\"].apply(lambda x:token.tokenize(x))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          text_clean  \n",
       "0  [our, deeds, are, the, reason, of, this, earth...  \n",
       "1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  [all, residents, asked, to, shelter, in, place...  \n",
       "3  [13000, people, receive, wildfires, evacuation...  \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing2(text):\n",
    "    text=[i for i in text if i not in stopwords.words(\"english\")]\n",
    "    lemmat =WordNetLemmatizer()\n",
    "    text= [lemmat.lemmatize(i) for i in text]\n",
    "    stemm = PorterStemmer()\n",
    "    text = \" \".join([stemm.stem(i) for i in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text[\"text_clean\"]  = dataset_text['text_clean'].apply(lambda x: text_preprocessing2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          text_clean  \n",
       "0           deed reason earthquak may allah forgiv u  \n",
       "1               forest fire near la rong sask canada  \n",
       "2  resid ask shelter place notifi offic evacu she...  \n",
       "3  13000 peopl receiv wildfir evacu order california  \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = train_test_split(dataset_text, test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          text_clean  \n",
       "0           deed reason earthquak may allah forgiv u  \n",
       "1               forest fire near la rong sask canada  \n",
       "2  resid ask shelter place notifi offic evacu she...  \n",
       "3  13000 peopl receiv wildfir evacu order california  \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_text.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Abdessamad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.tqdm(nltk.download('punkt'))\n",
    "punct = string.punctuation\n",
    "stopwords = nltk.corpus.stopwords.words()\n",
    "wl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameter\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_len = 1000\n",
    "trunc_type = \"post\"\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenzing the text\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(dataset_text['text_clean'])\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_word_sequence = tokenizer.texts_to_sequences(dataset_text['text_clean'])\n",
    "train_padd_sequence = pad_sequences(train_word_sequence, maxlen=max_len, truncating=trunc_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_dataset = pd.DataFrame(train_padd_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 1000)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3837</td>\n",
       "      <td>475</td>\n",
       "      <td>220</td>\n",
       "      <td>94</td>\n",
       "      <td>1407</td>\n",
       "      <td>3027</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "      <td>198</td>\n",
       "      <td>509</td>\n",
       "      <td>5567</td>\n",
       "      <td>5568</td>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>535</td>\n",
       "      <td>1902</td>\n",
       "      <td>395</td>\n",
       "      <td>5569</td>\n",
       "      <td>226</td>\n",
       "      <td>40</td>\n",
       "      <td>1902</td>\n",
       "      <td>395</td>\n",
       "      <td>348</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2508</td>\n",
       "      <td>14</td>\n",
       "      <td>2509</td>\n",
       "      <td>104</td>\n",
       "      <td>40</td>\n",
       "      <td>348</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>1071</td>\n",
       "      <td>151</td>\n",
       "      <td>3838</td>\n",
       "      <td>1541</td>\n",
       "      <td>211</td>\n",
       "      <td>104</td>\n",
       "      <td>2510</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  990   991   992  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...    0     0     0   \n",
       "1    0    0    0    0    0    0    0    0    0    0  ...    0     0     0   \n",
       "2    0    0    0    0    0    0    0    0    0    0  ...  535  1902   395   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...    0     0     0   \n",
       "4    0    0    0    0    0    0    0    0    0    0  ...    0    59  1071   \n",
       "\n",
       "    993   994   995   996   997   998   999  \n",
       "0  3837   475   220    94  1407  3027     8  \n",
       "1   147     4   198   509  5567  5568  1070  \n",
       "2  5569   226    40  1902   395   348   435  \n",
       "3  2508    14  2509   104    40   348    50  \n",
       "4   151  3838  1541   211   104  2510   131  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features_dataset, dataset_text['target'], test_size = 0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1002, 16)          160000    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 1002, 128)         41472     \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 308,609\n",
      "Trainable params: 308,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_len+2),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Créons un learning rate schedule pour décroitre le learning rate à mesure que nous entrainons le modèle \n",
    "initial_learning_rate = 0.0001\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=6000,\n",
    "    decay_rate=0.90,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(lr_schedule),\n",
    "              loss= tf.keras.losses.binary_crossentropy,\n",
    "              metrics = [tf.keras.metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1002) for input Tensor(\"embedding_3_input:0\", shape=(None, 1002), dtype=float32), but it was called on an input with incompatible shape (None, 1000).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1002) for input Tensor(\"embedding_3_input:0\", shape=(None, 1002), dtype=float32), but it was called on an input with incompatible shape (None, 1000).\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6822 - binary_accuracy: 0.5703WARNING:tensorflow:Model was constructed with shape (None, 1002) for input Tensor(\"embedding_3_input:0\", shape=(None, 1002), dtype=float32), but it was called on an input with incompatible shape (None, 1000).\n",
      "191/191 [==============================] - 327s 2s/step - loss: 0.6822 - binary_accuracy: 0.5703 - val_loss: 0.6742 - val_binary_accuracy: 0.5680\n",
      "Epoch 2/5\n",
      "191/191 [==============================] - 339s 2s/step - loss: 0.6171 - binary_accuracy: 0.6524 - val_loss: 0.5382 - val_binary_accuracy: 0.7630\n",
      "Epoch 3/5\n",
      "191/191 [==============================] - 347s 2s/step - loss: 0.4261 - binary_accuracy: 0.8190 - val_loss: 0.4963 - val_binary_accuracy: 0.7879\n",
      "Epoch 4/5\n",
      "191/191 [==============================] - 348s 2s/step - loss: 0.3585 - binary_accuracy: 0.8580 - val_loss: 0.5003 - val_binary_accuracy: 0.7853\n",
      "Epoch 5/5\n",
      "191/191 [==============================] - 349s 2s/step - loss: 0.3062 - binary_accuracy: 0.8856 - val_loss: 0.5106 - val_binary_accuracy: 0.7715\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
